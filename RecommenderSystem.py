# -*- coding: utf-8 -*-
"""Assessed_Coursework_2416218c.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jcd997Z2ytX85gR9ycHq3uy9FLZS5EZy

# Assessed Coursework

## Setup & Preparation
"""

!rm -rf ratings* books* to_read* test*

!curl -o ratings.csv "http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-ratings.csv" 
!curl -o books.csv "http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-books.csv"
!curl -o to_read.csv "http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-to_read.csv"
!curl -o test.csv "http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-test.csv"

#Standard setup
import pandas as pd
import numpy as np
import torch
!pip install git+https://github.com/maciejkula/spotlight.git@master#egg=spotlight
from spotlight.interactions import Interactions
ratings_df = pd.read_csv("ratings.csv")
books_df = pd.read_csv("books.csv")
to_read_df = pd.read_csv("to_read.csv")
test = pd.read_csv("test.csv")

from collections import defaultdict
from itertools import count
from spotlight.interactions import Interactions
from spotlight.cross_validation import random_train_test_split

#construct interactions from to_read_df
uid_map1 = defaultdict(count().__next__)
iid_map1 = defaultdict(count().__next__)
uids1 = np.array([uid_map1[uid] for uid in to_read_df["user_id"].values ], dtype=np.int32)
iids1 = np.array([iid_map1[iid] for iid in to_read_df["book_id"].values ], dtype=np.int32)
uid_rev_map1 = {v: k for k, v in uid_map1.items()}
iid_rev_map1 = {v: k for k, v in iid_map1.items()}
imp_dataset = Interactions(user_ids=uids1,
                                  item_ids=iids1, 
                                  num_users  = len(uid_map1),
                                  num_items = 7293 #len(iid_rev_map1)
                          )
imp_train, imp_validation = random_train_test_split(imp_dataset, test_percentage=0.2, random_state=np.random.seed(42)) 
print(imp_train)
print(imp_validation)

#construct interactions from test.csv
uid_map2 = defaultdict(count().__next__)
iid_map2 = defaultdict(count().__next__)
uids2 = np.array([uid_map2[uid] for uid in test["user_id"].values ], dtype=np.int32)
iids2 = np.array([iid_map2[iid] for iid in test["book_id"].values ], dtype=np.int32)
uid_rev_map2 = {v: k for k, v in uid_map2.items()}
iid_rev_map2 = {v: k for k, v in iid_map2.items()}
test_dataset = Interactions(user_ids=uids2,
                                  item_ids=iids2,
                                  num_users  = len(uid_map1),
                                  num_items = 7293 #len(iid_rev_map3)
                           )
print(test_dataset)

#construct interactions from ratings_df
uid_map3 = defaultdict(count().__next__)
iid_map3 = defaultdict(count().__next__)
uids3 = np.array([uid_map3[uid] for uid in ratings_df["user_id"].values ], dtype=np.int32)
iids3 = np.array([iid_map3[iid] for iid in ratings_df["book_id"].values ], dtype=np.int32)
uid_rev_map3 = {v: k for k, v in uid_map3.items()}
iid_rev_map3 = {v: k for k, v in iid_map3.items()}
ratings = ratings_df["rating"].values.astype(np.float32)
exp_dataset = Interactions(user_ids=uids3,
                                  item_ids=iids3,
                                  ratings=ratings,
                                  num_users  = len(uid_map3),
                                  num_items = 7293 #len(iid_rev_map3)
                          )
exp_train, exp_validation = random_train_test_split(exp_dataset, test_percentage=0.2, random_state=np.random.seed(42)) 
print(exp_train)
print(exp_validation)

#generate array of predicted results
def scoreAll(model):
  scoresForAllUsers=[]
  for uid in range(0,model._num_users):
    scoresForAllUsers.append(model.predict(uid))
  return scoresForAllUsers

#calculate Reciprocal Rank
import scipy.stats as st
def calc_reciprank(testInteractions, predictions_for_each_user, train=None, cutoff=0):
  FLOAT_MAX = np.finfo(np.float32).max
  assert testInteractions.num_users == len(predictions_for_each_user), "mismatch: num users in Interactions %d, num users in predictions was %d" % (testInteractions.num_users,len(predictions_for_each_user))
  assert testInteractions.num_items == len(predictions_for_each_user[0]), "mismatch: num items in Interactions %d, num items in predictions was %d" % (testInteractions.num_items, len(predictions_for_each_user[0]))
  
  rrs = []
  
  if train is not None:
    train = train.tocsr()
    
    i=-1
  for user_id, row in enumerate(testInteractions.tocsr()):
    i=i+1
    if not len(row.indices):
      rrs.append(0)
      continue
      
    predictions = -predictions_for_each_user[i]
    
    #if train is set, remove the interactions from the training dataset
    if train is not None:
      predictions[train[user_id].indices] = FLOAT_MAX
    
    rank = st.rankdata(predictions)[row.indices].min()
    
    if cutoff > 0 and rank > cutoff:
      rrs.append(0)
      continue
    
    rrs.append(1.0 / rank)
  return np.array(rrs)

"""## Question 1"""

#Average rating
book_groups = ratings_df.groupby('book_id')
avg_rating = []
for i in range(0, len(uid_map1)):
  avg_rating.append(book_groups['rating'].mean().as_matrix())
MRR_avg = calc_reciprank(test_dataset, avg_rating, train=exp_dataset).mean()
print("MRR of average rating: ", MRR_avg)

#Number of ratings
book_groups = books_df.groupby('book_id')
num_rating = []
for i in range(0, len(uid_map1)):
  num_rating.append(book_groups['ratings_count'].mean().as_matrix())
MRR_num_rating = calc_reciprank(test_dataset, num_rating, train=exp_dataset).mean()
print("MRR of number of ratings: ", MRR_num_rating)

#Number of 5* ratings
num_rating_5 = []
for i in range(0, len(uid_map1)):
  num_rating_5.append(book_groups['ratings_5'].mean().as_matrix())
MRR_num_rating_5 = calc_reciprank(test_dataset, num_rating_5, train=exp_dataset).mean()
print("MRR of number of 5* ratings: ", MRR_num_rating_5)

#Fraction of 5* ratings
book_groups = books_df.groupby('book_id')
fra_rating_5 = []
for i in range(0, len(uid_map1)):
  fra_rating_5.append((book_groups['ratings_5'].mean()/book_groups['ratings_count'].mean()).as_matrix())
RR_fra_rating_5 = calc_reciprank(test_dataset, fra_rating_5, train=exp_dataset)
MRR_fra_rating_5 = RR_fra_rating_5.mean()
print("MRR of fraction of 5* ratings: ", MRR_fra_rating_5)

"""####  So, the best model of Question 1 is Fraction of 5* ratings(fra_rating_5)

## Question 2
"""

from spotlight.factorization.explicit import ExplicitFactorizationModel
from spotlight.factorization.implicit import ImplicitFactorizationModel

#ExplicitFactorizationModel
emodel = ExplicitFactorizationModel(n_iter=10,
                                    embedding_dim=32, 
                                    use_cuda=False)
emodel.fit(exp_train, verbose=True)
score_emodel = scoreAll(emodel)
print(calc_reciprank(exp_validation, score_emodel, train=exp_train).mean())


#ImplicitFactorizationModel
imodel = ImplicitFactorizationModel(n_iter=10,
                                    loss='bpr',
                                    embedding_dim=32, 
                                    use_cuda=False)
imodel.fit(exp_train, verbose=True)
score_imodel_32_on_exp = scoreAll(imodel)
print(calc_reciprank(exp_validation, score_imodel_32_on_exp, train=exp_train).mean())

#ImplicitFactorizationModel is more effective
#tune the number of latent factors
imodel_64 = ImplicitFactorizationModel(n_iter=10,
                                       loss='bpr',
                                    embedding_dim=64, 
                                    use_cuda=False)
imodel_64.fit(exp_train, verbose=True)
print(calc_reciprank(exp_validation, scoreAll(imodel_64), train=exp_train).mean())

imodel_128 = ImplicitFactorizationModel(n_iter=10,
                                        loss='bpr',
                                    embedding_dim=128, 
                                    use_cuda=False)
imodel_128.fit(exp_train, verbose=True)
print(calc_reciprank(exp_validation, scoreAll(imodel_128), train=exp_train).mean())

#the best model is imodel, in which latent factor is 32 
RR_Q2 = calc_reciprank(test_dataset, scoreAll(imodel), train=exp_dataset)
MRR_Q2 = RR_Q2.mean()
print("MRR of ImplicitFactorizationModel with latent factor 32 is : ",MRR_Q2)

"""## Question 3"""

#deploy ImplicitFactorizationModel on implicit data
from spotlight.factorization.implicit import ImplicitFactorizationModel
imodel_imp = ImplicitFactorizationModel(n_iter=10,
                                    loss='bpr',
                                    embedding_dim=32, 
                                    use_cuda=False)
imodel_imp.fit(imp_train, verbose=True)
print(calc_reciprank(imp_validation, scoreAll(imodel_imp), train=imp_train).mean())

imodel_imp_64 = ImplicitFactorizationModel(n_iter=10,
                                    loss='bpr',
                                    embedding_dim=64, 
                                    use_cuda=False)
imodel_imp_64.fit(imp_train, verbose=True)
print(calc_reciprank(imp_validation, scoreAll(imodel_imp_64), train=imp_train).mean())

imodel_imp_128 = ImplicitFactorizationModel(n_iter=10,
                                    loss='bpr',
                                    embedding_dim=128, 
                                    use_cuda=False)
imodel_imp_128.fit(imp_train, verbose=True)
score_imodel_128_on_imp = scoreAll(imodel_imp_128)
print(calc_reciprank(imp_validation, scoreAll(imodel_imp_128), train=imp_train).mean())

#the best model is imodel_imp,  in which the latent factor is 32
predicted_scores_for_all = scoreAll(imodel_imp)
RR_32 = calc_reciprank(test_dataset, predicted_scores_for_all, train=imp_train)
MRR_32 = RR_32.mean()
print("MRR of ImplicitFactorizationModel with latent factor 32 is : ",MRR_32)

predicted_scores_for_all = scoreAll(imodel_imp_64)
RR_64 = calc_reciprank(test_dataset, predicted_scores_for_all, train=imp_train)
MRR_64 = RR_64.mean()
print("MRR of ImplicitFactorizationModel with latent factor 64 is : ",MRR_64)

predicted_scores_for_all = scoreAll(imodel_imp_128)
RR_128 = calc_reciprank(test_dataset, predicted_scores_for_all, train=imp_train)
MRR_128 = RR_128.mean()
print("MRR of ImplicitFactorizationModel with latent factor 128 is : ",MRR_128)

#pick userids with highest RR scores and analyze the number of their previously ratings, top 20 predicted books and their actually added books
RR_ranks = st.rankdata(-RR_128, method='min')

def countRatings(uid, rr):
  list = []
  count = 0
  print("User with id %d with RR %f had %d ratings" %( uid, rr, len(iids1[uids1==uid])))
    
  print("predicted top 20 books.")
  ranks = st.rankdata(-imodel_imp_128.predict(uid))  
  for iid in  np.argwhere(ranks <= 20):
    list.append(iid_rev_map1[iid.item(0)])

  print("Actually added ", test[test["user_id"]==uid_rev_map2.get(uid)]["book_id"].count(),"books.")
  for value in test[test["user_id"]==uid_rev_map2.get(uid)]["book_id"].values:
    for i in range(0, len(list)):
      if(list[i] == value):
        count += 1
  print(count, " books predicted successfully!")

for i in range(0, len(np.argwhere(RR_ranks==1))):
  countRatings( np.argwhere(RR_ranks==1).item(i), RR_128[RR_ranks==1].item(i) )

#list top 5 scored items of a given user
#obtain their item embeddings
import scipy.stats as st
vectors = []
def tracksForUser(uid, k=5):
  ranks = st.rankdata(-imodel_imp_128.predict(uid))  
  for iid in np.argwhere(ranks <= k):
    print(iid_rev_map1[iid.item(0)])
    i = iid_rev_map1[iid.item(0)]
    print(imodel_imp._net.item_embeddings.weight[i])
    vectors.append(imodel_imp._net.item_embeddings.weight[i])
    
tracksForUser(86)

import torch.nn as nn
#calculate intra-list diversity of these 5 items
def calc_ild(list):
  cosine = 0
  for i in range(0,4):
    for j in range(i+1,4):
      cosine += nn.functional.cosine_similarity(list[i], list[j], dim=0)
  return (2 / (5 * 4) * cosine)

cosine = calc_ild(vectors)
print(cosine)

"""## Question 4"""

import matplotlib.pyplot as plt
score_Q2 = scoreAll(imodel)
comb_sum = []
for i in range(0, len(score_Q2)):
  temp = []
  for j in range(0, len(score_Q2[0])):
    temp.append(fra_rating_5[i][j] + score_Q2[i][j])
  temp_2 =  np.asarray(temp)
  comb_sum.append(temp_2)
  
RR_Q4 = calc_reciprank(test_dataset, comb_sum, train=exp_dataset)
MRR_Q4 = RR_Q4.mean()
print(MRR_Q4)

from matplotlib.pyplot import figure
fig = plt.figure(num=None, figsize=(500, 6), dpi=80, facecolor='w', edgecolor='k')
ax = fig.subplots()
ax.plot(RR_fra_rating_5, 'y', label = 'Q1')
ax.plot(RR_Q2, 'b', label = 'Q2')
ax.plot(RR_Q4, 'r', label = 'Q4')
ax.legend()
plt.xlim([0,5000])
plt.show()

"""## Question 5

### LSA content-based recommender based on the aggregated user representation
"""

#construct book feature matrix
#extract titles and authors from books_df
corpus = []
for title,author in books_df[["title","authors"]].values:
  corpus.append(title + " " +author)

#create one-hot encoding representation (text_rep)
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(binary = True)
text_rep = vectorizer.fit_transform(corpus)

#deploy LSA to reduce dimensionalty  (dense_rep_cvt)
from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=20,n_iter=7, random_state=42)
dense_rep = svd.fit_transform(text_rep)  
dense_rep_cvt = np.asarray(dense_rep)
print("size of book feature matrix :", dense_rep_cvt.shape)

#constrcut utility matrix
new_df =  ratings_df[ratings_df['rating'] > 3]

util_matrix = [[0 for col in range(len(dense_rep))] for row in range(len(uid_map1))]

for i in range(0, len(uid_map1)):
    for book_id in new_df[new_df['user_id'] == i][["book_id"]].values:
      j = iid_map3[book_id[0]] 
      util_matrix[i][j] = 1  
print("size of utility matrix :", len(util_matrix), " x " ,len(util_matrix[0]))

#compute  user feature matrix using book feature matrix and utility matrix
user_fea_matrix = [[0 for col in range(len(uid_map1))] for row in range(len(dense_rep[0]))]
for i in range(0, len(dense_rep[0])):
  for j in range(0, len(uid_map1)):
    user_fea_matrix[i][j] = sum(util_matrix[j] * dense_rep[:,i])
    
user_fea_matrix_cvt = np.asarray(user_fea_matrix)
print("size of user feature matrix :", user_fea_matrix_cvt.shape)

#predict ratings using user feature matrix and book feature matrix
#compute the cosine similarity for all books to each user
for i in range(0, len(uid_map1)):
  for j in range(0, len(dense_rep)):
    if(util_matrix[i][j] == 0):
      util_matrix[i][j] = sum(user_fea_matrix_cvt[:,i] * dense_rep_cvt[j]) / (np.linalg.norm(user_fea_matrix_cvt[:,i]) * np.linalg.norm(dense_rep_cvt[j]))  

util_matrix_cvt = []
for i in range(0, len(uid_map1)):
  temp = np.asarray(util_matrix[i])
  util_matrix_cvt.append(temp)
  
#evaluate in terms of MRR
MRR_Q5_a = calc_reciprank(test_dataset, util_matrix_cvt, train=exp_dataset).mean()
print("MRR of Q5-a is : ", MRR_Q5_a)

"""### LSA content-based recommender based on the weighting-rated cosine"""

#constrcut utility matrix, based on users' ratings (3, 4, 5)
new_df =  ratings_df[ratings_df['rating'] >= 3]

util_matrix = [[0 for col in range(len(dense_rep))] for row in range(len(uid_map1))]
user_rating_count = []

for i in range(0, len(uid_map1)):
    for book_id, rating in new_df[new_df['user_id'] == i][["book_id", "rating"]].values:
      j = iid_map3[book_id]
      util_matrix[i][j] = rating 
    user_rating_count.append(new_df[new_df['user_id'] == i]['rating'].count())
print("size of utility matrix :", len(util_matrix), " x " ,len(util_matrix[0]))

#construct user feature matrix using utility matrix and book feature matrix
user_fea_matrix_b = [[0 for col in range(len(uid_map1))] for row in range(len(dense_rep[0]))]

for i in range(0, len(dense_rep[0])):
  for j in range(0, len(uid_map1)):
    user_fea_matrix_b[i][j] = sum(util_matrix[j] * dense_rep[:,i]) / user_rating_count[j]
    
user_fea_matrix_cvt_b = np.asarray(user_fea_matrix_b)
print("size of user feature matrix :", user_fea_matrix_cvt_b.shape)

#compute weighted cosine similarity using book feature matrix and user feature matrix
for i in range(0, len(uid_map1)):
  for j in range(0, len(dense_rep)):
    if(util_matrix[i][j] == 0):
      util_matrix[i][j] = sum(user_fea_matrix_cvt_b[:,i] * dense_rep_cvt[j]) / (np.linalg.norm(user_fea_matrix_cvt_b[:,i]) * np.linalg.norm(dense_rep_cvt[j]))  

util_matrix_cvt_b = []
for i in range(0, len(uid_map1)):
  temp = np.asarray(util_matrix[i])
  util_matrix_cvt_b.append(temp)
  
#evaluate in terms of MRR
MRR_Q5_b = calc_reciprank(test_dataset, util_matrix_cvt_b, train=exp_dataset).mean()
print("MRR of Q5-b is : ", MRR_Q5_b)

"""## Question 6"""

score_LSA_cb_2 = util_matrix_cvt_b

"""### Attemp 1"""

#input features to be used:
#Fractino of 5* ratings: fra_rating_5                                         x1
#ImplicitFactorizationModel on explicit data (32): score_imodel_32_on_exp     x2 
#ImplicitFactorizationModel on implicit data(128): score_imodel_128_on_imp    x2
#LSA content-based recommender 2: score_LSA_cb_2                              x1

comb_sum_1 = []
for i in range(0, len(fra_rating_5)):
  temp = []
  for j in range(0, len(fra_rating_5[0])):
    temp.append(fra_rating_5[i][j] + score_imodel_32_on_exp[i][j]*2 + score_imodel_128_on_imp[i][j]*2 + score_LSA_cb_2[i][j])
  temp_2 =  np.asarray(temp)
  comb_sum_1.append(temp_2)
  
RR_1 = calc_reciprank(imp_validation, comb_sum_1, train=imp_train)
MRR_1 = RR_1.mean()
print("MRR of attempt 1 is : ", MRR_1)

"""### Attemp 2"""

#input features to be used:
#Fractino of 5* ratings: fra_rating_5                                         x2
#ImplicitFactorizationModel on explicit data (32): score_imodel_32_on_exp     x4 
#ImplicitFactorizationModel on implicit data(128): score_imodel_128_on_imp    x5
#LSA content-based recommender 2: score_LSA_cb_2                              x2

comb_sum_2 = []
for i in range(0, len(fra_rating_5)):
  temp = []
  for j in range(0, len(fra_rating_5[0])):
    temp.append(fra_rating_5[i][j]*2 + score_imodel_32_on_exp[i][j]*4 + score_imodel_128_on_imp[i][j]*5 + score_LSA_cb_2[i][j]*2)
  temp_2 =  np.asarray(temp)
  comb_sum_2.append(temp_2)
  
RR_2 = calc_reciprank(imp_validation, comb_sum_2, train=imp_train)
MRR_2 = RR_2.mean()
print("MRR of attempt 2 is : ", MRR_2)

"""### Attemp 3"""

#input features to be used:
#Fractino of 5* ratings: fra_rating_5                                         x1
#ImplicitFactorizationModel on explicit data (32): score_imodel_32_on_exp     x2 
#ImplicitFactorizationModel on implicit data(128): score_imodel_128_on_imp    x2
#LSA content-based recommender 2: score_LSA_cb_2                              x1
#ExplicitFactorizationModel : score_emodel                                    x1

comb_sum_3 = []
for i in range(0, len(fra_rating_5)):
  temp = []
  for j in range(0, len(fra_rating_5[0])):
    temp.append(fra_rating_5[i][j]*1 + score_imodel_32_on_exp[i][j]*2+ score_imodel_128_on_imp[i][j]*2 + score_LSA_cb_2[i][j]*2 + score_emodel[i][j]*1)
  temp_2 =  np.asarray(temp)
  comb_sum_3.append(temp_2)
  
RR_3 = calc_reciprank(imp_validation, comb_sum_3, train=imp_train)
MRR_3 = RR_3.mean()
print("MRR of attempt 3 is : ", MRR_3)

"""### the best model with test dataset"""

RR_final = calc_reciprank(test_dataset, comb_sum_2, train=imp_dataset)
MRR_final = RR_final.mean()
print(" Final MRR performance is : ", MRR_final)